\chapter{Research questions and Challenges}
Smart Grid generates a significant amount of data due to large scale deployment of digital technologies in distribution network on a daily basis, and these data must be processed in an efficient way in order to monitor the operational status of the grid. Research has shown that many existing techniques in the field of data analytics have not fully explored the value of data. However, serious attention has been given to this field and some well-known institutions have updated their teaching and research programs to higher education students who can take up the challenges of big data research and applications in the near future. Data analytics competitors are competing to bring a set of IT tools and capabilities that are largely new to the utility industry \cite{lai2015big}. 

Information is necessary for energy players, and for decision-makers and regulators who must establish rules and mechanisms to provide a framework for a competitive market and allow smart grids to achieve optimal efficiency \cite{clastres2011smart}. With the worldwide initiative of upgrading the traditional electrical grid to the Smart Grid, new challenges have been introduced, among which is the “Big Data” challenge. Many researchers are asking themselves what they can do to harness this deluge of data to make more intelligent operational and business decisions. 

With smart grids, it will be possible to obtain accurate, real-time data on consumption profiles and the status of electricity systems, with scope for isolating certain items on which it is appropriate to act (heating, household devices, etc.). Measures to promote energy efficiency and control consumption will benefit from the new behavioural data. The positive effect expected of smart grids is not necessarily a drop in prices but rather a reduction in the bills paid by consumers. It is estimated that gains from optimizing or reducing consumption will counterbalance higher prices due to regulation of and higher security on the electricity system\cite{clastres2011smart}. 

Regarding data analytics applications to the smart grid, the key challenges are focused on converting the tens of billions of data points coming from the millions of smart meters deployed around the country and turning them into actionable information for the grid operations. The data could generate information on how individual customers respond to requests of consumption reduction. It is also possible to use real-time metering data to discover unaccounted consumption when energy is being diverted and stolen. The aim is to improve grid reliability, outage response and reducing the cost of distribution operations. Occasionally emerged frauds or intrusions in smart grid systems have incurred significant loss when the suspicious activities were not detected or inefficiently processed. Therefore we found some of the research questions and challenges which needs to be answered to efficiently deploy the anomaly detection methods for electrical data:

\begin{enumerate}
\item \textbf{Availability of sufficient and valid input data:} For any machine learning algorithms to work and perform with good accuracy, is dependent on the size of input data. Sufficient amount of data will provide us with prior knowledge about the relationship between the variables, without which it will be difficult to generalize the unseen data. In our case of predicting unexpected events, working with small set of data could be challenging, as we may not capture required dependencies between the variables in the data, given the data is not large enough. We limit ourselves to only the size of the data in this discussion and won't be discussing about how the data is collected, assuming it is collected efficiently and systematically. It is important to know what is the data used for analysis, keeping in mind the purpose of the data and about the learning outcomes of it. Next, how many data points are enough to provide the evidence of the outcome achievement. Further details of how much data is enough can be found in \cite{rogers2002death}

\item \textbf{Dealing with unlabeled data:} The electrical data used for our analysis do not contain labels for anomalies and hence we use the general term "unlabeled data" as used in machine learning. We proceed by considering that most part of the data contain normal patterns and only a small part contain anomalous patterns. This assumption will be proved right or wrong later in the evaluation section after modeling and training the data. There is no explanation for each piece of data, it just contains data and nothing else. Challenge arises in determining which data to be considered as normal and which data as anomaly since we do not have labels or classes to distinguish. Dealing with unlabeled data also challenges the choice of algorithm to be implemented which works the best with these data. Sometimes it can be taken into account that the accuracy in prediction will be high when using enough amount unlabeled data than labeled data \cite{liang2007use}.

\item \textbf{Identifying useful information:}The next challenge arises is feature selection. Identifying only the relevant features is important part of machine learning in anomaly detection. We need to take care of removing unnecessary data such as duplicates in the data which may be caused due to inefficient measurements and data containing 'null' values. Cleaning the data for removal of duplicates, null values and also to remove noise will help to analyse the data efficiently. Data Smoothing, by considering moving average windows readily available in python libraries can be used to do this cleaning process.

\item \textbf{Dimensionality Reduction:} Dimensionality reduction is used as an efficient technique when considering high dimensional datasets. Our anomaly detection process will be hard if we do not consider reducing the data to low dimension. To retain all the necessary information without losing any important aspects of the data will be challenging. For this we will employ well known dimensionality reduction machine learning technique such as PCA. PCA will reduce the dimensionality in the data considering only relevant information representing the whole data and ignoring the information which do not effect our anomaly detection process and which do not lose any important information about the data.

\item \textbf{Determining a suitable model of normality:} How to model the data will be the biggest challenge in our work. Modelling the data will determine the performance of the process of detecting the anomalies as well to describe the process efficiently. Model which best suit our data should be utilized. We will start with the basic and well known Gaussian Mixture Models and then PCA with residual parameters, followed by one of the efficient deep learning model, LSTM (Long Short-term Memory).
 
\item \textbf{Train the model:}Simply turning the work over to machines won’t help: most machine learning and statistical mining techniques also hold the assumption that historical data, which is used to train the machine-learning model, behaves similarly to the target data, to which the model is later applied. We should be able to determine, what is the percentage of data to be used for training, validating and testing the model. The training data should be able to capture all the necessary features of the data which will be used to predict the future events. Choosing the training data will directly effect the accuracy of our model, 

\item \textbf{Finding optimal model parameters:}  Identify suitable statistical metric with which we will prove the correctness of our model. This statistical metric should be able to calculate the threshold with which our model distinguishes between normal and abnormal patterns. This threshold value will the borderline between normal behavior and anomalous behavior. Choose a proper threshold above or below which data can be considered as anomaly/ies. Measures have to be taken care while fitting the model. A good model for the data should not overfit by using very large model parameters and it should not underfit as well using very less model parameters. Determining proper model parameters to include huge amount of data poses challenges to avoid the number of false positives and false negatives. False positives are data which are detected as anomalous are not truly anomalous and False negatives are data which are anomalous but our model did not detect.

\item \textbf{Deployment Efforts:} Great efforts have to be spent to develop more advanced and efficient algorithms for data analysis. Training the model with sufficient data, validating it and testing our methods consumed the most part of our work. 
\end{enumerate}
\label{sec:Research questions and Challenges}